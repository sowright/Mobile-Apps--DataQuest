{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google and Apple Apps Data Analyis\n",
    "\n",
    "We look at data from Google and Apple apps, with the goal of answering this question: Which types of apps are most likely to attract the most users on both platforms?\n",
    "\n",
    "In this notebook we download and clean the data. In the next notebook we will analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Source\n",
    "\n",
    "We downloaded the Google apps data from [here](https://www.kaggle.com/lava18/google-play-store-apps/version/6) and the Apple apps data from [here](https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv.\n",
    "\n",
    "from csv import reader\n",
    "\n",
    "open_file_apple = open('AppleStore.csv', encoding='utf8') \n",
    "read_file_apple = reader(open_file_apple)\n",
    "apple_data = list(read_file_apple)\n",
    "\n",
    "open_file_google = open('googleplaystore.csv', encoding='utf8')\n",
    "read_file_google = reader(open_file_google)\n",
    "google_data = list(read_file_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Examine the Data\n",
    "\n",
    "Let's look at the number of rows and columns in each data set, and a few sample rows to see what the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a dataset, and the start and end index for a slice of rows to examine. \n",
    "# The output will only display the number of rows and columns if rows_and_columns input is True.\n",
    "# If the input data has a header, we want to account for that in the row count. \n",
    "\n",
    "def explore_data(dataset, start, end, rows_and_columns=False, header=False):  \n",
    "    \n",
    "    if rows_and_columns:\n",
    "        # print number of rows\n",
    "        if header:\n",
    "            print('Number of rows with data:', len(dataset)-1) # account for header row by subtracting one from total row count\n",
    "        else:\n",
    "            print('Number of rows:', len(dataset))\n",
    "        \n",
    "        # print number of columns\n",
    "        print('Number of columns:', len(dataset[0]))\n",
    "        print('\\n')\n",
    "    \n",
    "    dataset_slice = dataset[start:end]\n",
    "    for row in dataset_slice:\n",
    "        print(row)\n",
    "        print('\\n') # adds a new (empty) line after each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with data: 10841\n",
      "Number of columns: 13\n",
      "\n",
      "\n",
      "['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver']\n",
      "\n",
      "\n",
      "['Photo Editor & Candy Camera & Grid & ScrapBook', 'ART_AND_DESIGN', '4.1', '159', '19M', '10,000+', 'Free', '0', 'Everyone', 'Art & Design', 'January 7, 2018', '1.0.0', '4.0.3 and up']\n",
      "\n",
      "\n",
      "['Coloring book moana', 'ART_AND_DESIGN', '3.9', '967', '14M', '500,000+', 'Free', '0', 'Everyone', 'Art & Design;Pretend Play', 'January 15, 2018', '2.0.0', '4.0.3 and up']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore Google data\n",
    "# Our data sets both have headers, so we will specify \"header = True\" in the input for both.\n",
    "\n",
    "explore_data(google_data, 0, 3, rows_and_columns = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with data: 7197\n",
      "Number of columns: 16\n",
      "\n",
      "\n",
      "['id', 'track_name', 'size_bytes', 'currency', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic']\n",
      "\n",
      "\n",
      "['284882215', 'Facebook', '389879808', 'USD', '0.0', '2974676', '212', '3.5', '3.5', '95.0', '4+', 'Social Networking', '37', '1', '29', '1']\n",
      "\n",
      "\n",
      "['389801252', 'Instagram', '113954816', 'USD', '0.0', '2161558', '1289', '4.5', '4.0', '10.23', '12+', 'Photo & Video', '37', '0', '29', '1']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore Apple data\n",
    "\n",
    "explore_data(apple_data, 0, 3, rows_and_columns = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Cleaning\n",
    "\n",
    "We want to:\n",
    "- Remove incorrect or incomplete data.\n",
    "- Remove duplicates.\n",
    "- Limit data to Enlglish-language apps.\n",
    "- Limit data to free apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.a  Remove incomplete data\n",
    "\n",
    "An entry in the Google dataset with incomplete data is called out in [this thread](https://www.kaggle.com/lava18/google-play-store-apps/discussion/66015) in the discussion forum. We will check both data sets for rows that are shorter than the header - an indication that they are missing data - and delete those rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver']\n",
      "\n",
      "\n",
      "row index: 10473\n",
      "['Life Made WI-Fi Touchscreen Photo Frame', '1.9', '19', '3.0M', '1,000+', 'Free', '0', 'Everyone', '', 'February 11, 2018', '1.0.19', '4.0 and up']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check row length discrepancies in google data\n",
    "\n",
    "n_columns = len(google_data[0])\n",
    "\n",
    "print(google_data[0])\n",
    "print('\\n')\n",
    "\n",
    "for row in google_data:\n",
    "    if len(row) < n_columns:\n",
    "        print('row index: ' + str(google_data.index(row)))\n",
    "        print(row)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'track_name', 'size_bytes', 'currency', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check row length discrepancies in apple data\n",
    "\n",
    "n_columns = len(apple_data[0])\n",
    "\n",
    "print(apple_data[0])\n",
    "print('\\n')\n",
    "\n",
    "for row in apple_data:\n",
    "    if len(row) < n_columns:\n",
    "        print('row index: ' + str(apple_data.index(row)))\n",
    "        print(row)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified the same row in the Google data that that discussion forum identified; we delete that in the next cell. We did not find any incomplete rows in the Apple data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete row 10473 from google data. \n",
    "# NOTE: we only want to delete this once; do not run this row multiple times.\n",
    "\n",
    "del google_data[10473]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.b Remove duplicates\n",
    "\n",
    "We would like to have only one row per app. To do this we first need to find the duplicate rows, then delete them.\n",
    "\n",
    "In some cases two rows for the same app are exactly the same, in which case it does not matter which row we keep. In other cases, two rows for the same app may have differing information in other fields. To determine which entry to keep, we decided to keep the row with the highest number of reviews - it is most likely that the entry with the higher number of reviews is the more recent entry.\n",
    "\n",
    "We will do this in a few steps:\n",
    "1. **Define a function that identifies duplicates in a given data set.** This function returns both a list of unique app names and a list of the duplicate app names.\n",
    "2. **Build a dictionary that stores each app name with the highest number of reviews listed for that app in one entry.** We use this as a proxy for the most recent entry for each app. We also write a function for this step. The resulting dictionary has app names as keys and review counts as values.\n",
    "3. **Create a new data set of unique values by keeping only the entries that match the highest number of reviews for that app, and no identical entries.** We define a function that iterates over our dataset and compares the review count with the app name's corresponding value in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function that identifies duplicates in a given data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the duplicates in a given data set.\n",
    "# Since the app name is in a different column in our two data sets, the function also takes in the index of column that holds the app name.\n",
    "# The function will return two lists: one of unique app names and one of all duplicate entry names.\n",
    "\n",
    "def find_duplicate_apps(data, index_app_name):\n",
    "    unique_apps = []\n",
    "    duplicate_apps = []\n",
    "    \n",
    "    for row in data:\n",
    "        app_name = row[index_app_name]\n",
    "        if app_name in unique_apps:\n",
    "            duplicate_apps.append(app_name)\n",
    "        else:\n",
    "            unique_apps.append(app_name)\n",
    "    \n",
    "    return unique_apps, duplicate_apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181\n"
     ]
    }
   ],
   "source": [
    "# Count the number of duplicate rows in the google data.\n",
    "# Note, we use index 0 as input since that is the column with the app names.\n",
    "\n",
    "google_duplicates = find_duplicate_apps(google_data, 0)[1]\n",
    "print(len(google_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Slack', 'BUSINESS', '4.4', '51507', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'August 2, 2018', 'Varies with device', 'Varies with device']\n",
      "\n",
      "\n",
      "['Slack', 'BUSINESS', '4.4', '51507', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'August 2, 2018', 'Varies with device', 'Varies with device']\n",
      "\n",
      "\n",
      "['Slack', 'BUSINESS', '4.4', '51510', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'August 2, 2018', 'Varies with device', 'Varies with device']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at an example of a duplicated app in the google data.\n",
    "# From examining the list stored in google_duplicates, I know that the \"Slack\" app has more than one entry.\n",
    "\n",
    "for row in google_data:\n",
    "    if row[0] == 'Slack':\n",
    "        print(row)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of duplicate rows in the apple data.\n",
    "# This time we use index 1 for input as that is the column with the app names in this dataset. \n",
    "\n",
    "len(find_duplicate_apps(apple_data, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a dictionary that stores each app name with the highest number of reviews listed for that app in one entry.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that builds a dictionary to track the greatest review count for each app.\n",
    "# The keys will be the the app names and the values will be the number of reviews.\n",
    "\n",
    "def build_max_reviews_dict(data, index_app_name, index_num_reviews):\n",
    "    max_reviews_dict = {}\n",
    "    unique_apps = find_duplicate_apps(data, index_app_name)[0] # Use our defined function.\n",
    "    \n",
    "    for row in data[1:]:\n",
    "        app_name = row[index_app_name]\n",
    "        number_reviews = float(row[index_num_reviews])\n",
    "\n",
    "        if app_name in unique_apps:\n",
    "            if (app_name not in max_reviews_dict.keys()) or (number_reviews > max_reviews_dict[app_name]):\n",
    "                max_reviews_dict[app_name] = number_reviews\n",
    "    \n",
    "    return max_reviews_dict\n",
    "\n",
    "#max_reviews_dict = {}\n",
    "#google_unique_apps = find_duplicate_apps(google_data, 0)[0] # Use our defined function.\n",
    "\n",
    "#for row in google_data[1:]:\n",
    "#    app_name = row[0]\n",
    "#    number_reviews = float(row[3])\n",
    "#    \n",
    "#    if app_name in google_unique_apps:\n",
    "#        if (app_name not in max_reviews_dict.keys()) or (number_reviews > max_reviews_dict[app_name]):\n",
    "#            max_reviews_dict[app_name] = number_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10841\n",
      "9659\n"
     ]
    }
   ],
   "source": [
    "google_max_reviews_dict = build_max_reviews_dict(google_data, 0, 3)\n",
    "\n",
    "print(len(google_data))\n",
    "print(len(google_max_reviews_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7198\n",
      "7195\n"
     ]
    }
   ],
   "source": [
    "apple_max_reviews_dict = build_max_reviews_dict(apple_data, 1, 5)\n",
    "\n",
    "print(len(apple_data))\n",
    "print(len(apple_max_reviews_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new data set of unique values by keeping only the entries that match the highest number of reviews for that app, and no identical entries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data set without duplicate rows, keeping only the most recent entry for each app \n",
    "# (we decided the most recent entryn is the one that has the highest number of reviews)\n",
    "\n",
    "def remove_duplicates(data, index_app_name, index_num_reviews):\n",
    "    data_dedup = []\n",
    "    header = data[0]\n",
    "    data_dedup.append(header)\n",
    "    \n",
    "    already_listed = []\n",
    "    \n",
    "    # Use our defined function to build a dictionary of max reviews number.\n",
    "    max_reviews_dict = build_max_reviews_dict(data, index_app_name, index_num_reviews)\n",
    "    \n",
    "    # Iterate over our data and add rows to out new list if they match the max reviews number\n",
    "    #   AND if they do not already exist in the new list.\n",
    "    for row in data[1:]:\n",
    "        app_name = row[index_app_name]\n",
    "        number_reviews = float(row[index_num_reviews])\n",
    "        \n",
    "        if (max_reviews_dict[app_name] == number_reviews) and (app_name not in already_listed):\n",
    "            data_dedup.append(row)\n",
    "            already_listed.append(app_name)\n",
    "            \n",
    "    return data_dedup\n",
    "\n",
    "#google_data_new = []\n",
    "#header = google_data[0]\n",
    "#google_data_new.append(header)\n",
    "#\n",
    "#already_listed = []\n",
    "#\n",
    "#for row in google_data[1:]:\n",
    "#    app_name = row[0]\n",
    "#    number_reviews = float(row[3])\n",
    "#    \n",
    "#    if (max_reviews_dict[app_name] == number_reviews) and (app_name not in already_listed):\n",
    "#        google_data_new.append(row)\n",
    "#        already_listed.append(app_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9660"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_data_dedup = remove_duplicates(google_data, 0, 3)\n",
    "\n",
    "len(google_data_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7196"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_data_dedup = remove_duplicates(apple_data, 1, 5)\n",
    "\n",
    "len(apple_data_dedup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's confirm that our functions worked and that there are no duplicates left in either \"deduped\" data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_duplicate_apps(google_data_dedup, 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_duplicate_apps(google_data_dedup, 0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.c  Limit to English-language apps\n",
    "\n",
    "We are only interested in including English-language apps for this analysis. One way to approach this is to remove any app that has non-English characters in the app name. \n",
    "\n",
    "(Note that this will not necessarily catch apps that are in Spanish, French, or other languages that use predominanely the same alphabet as English, but for the sake of this excercise we will use this approach.)\n",
    "\n",
    "Our process will take two steps:\n",
    "1. **Define a function that determines if an app name has more than three non-English characters.** We will iterate over the string to do this, and use the ASCII number of each character to determine if it is in the English alphabet or not. We set the threshold of at least three characters so that emojis and other irregular characters may be permitted if they only occur a few times. \n",
    "2. **Apply this function to each row, and remove rows that are not determined to have app names in English**. In order to preserve our data set, rather than delete these rows we will create a new data set for the desired English-only apps. Again, we write a function for this so that it will be easy to apply to both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_string_english(string):\n",
    "    # keep count of non-English characters in the string\n",
    "    non_english_char_count = 0\n",
    "    \n",
    "    # iterate over the string\n",
    "    for character in string:\n",
    "        ascii_num = ord(character)  # use the built-in function ord() to get the ASCII number of a character.\n",
    "        if ascii_num > 127:  # standard English-language characters will have ASCII number between 0 and 127.\n",
    "            non_english_char_count += 1\n",
    "        if non_english_char_count == 3:\n",
    "            return False  # if the string has more than 3 non-English characters, we determine that the name is not in English.\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_english_apps(data, index_app_name):\n",
    "    data_eng = [data[0]]   \n",
    "    # eng for \"English\". I realize in retrospect that this looks like \"Data Engineering\"...\n",
    "\n",
    "    for row in data[1:]:\n",
    "        app_name = row[0]\n",
    "        if is_string_english(app_name):\n",
    "            data_eng.append(row)\n",
    "        \n",
    "    return data_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9598"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_data_eng = only_english_apps(google_data_dedup, 0)\n",
    "\n",
    "len(google_data_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7196"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_data_eng = only_english_apps(apple_data_dedup, 0)\n",
    "\n",
    "len(apple_data_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.d  Limit to Free Apps\n",
    "\n",
    "We are only interested in analyzing apps that are free. In this final dat transformation, we will create a new data set that keeps only the apps for which price = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_free_apps(data, index_price):\n",
    "    data_free = [data[0]]\n",
    "    \n",
    "    for row in data[1:]:\n",
    "        price = float(row[index_price].replace('$',''))\n",
    "        if price == 0:\n",
    "            data_free.append(row)\n",
    "    \n",
    "    return data_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9598\n",
      "8849\n"
     ]
    }
   ],
   "source": [
    "google_data_free = only_free_apps(google_data_eng, 7)\n",
    "\n",
    "print(len(google_data_eng))\n",
    "print(len(google_data_free))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7198\n",
      "4055\n"
     ]
    }
   ],
   "source": [
    "apple_data_free = only_free_apps(apple_data_eng, 4)\n",
    "\n",
    "print(len(apple_data))\n",
    "print(len(apple_data_free))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Save final data sets to csv\n",
    "\n",
    "We will use these transformed data sets for analysis in the next notebook.\n",
    "\n",
    "We follow these instructions [here](https://www.programiz.com/python-programming/writing-csv-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "\n",
    "with open('google_data_transformed.csv', 'w', encoding=\"utf-8\", newline='') as file:\n",
    "    writer_obj = writer(file)\n",
    "    writer_obj.writerows(google_data_free)\n",
    "\n",
    "with open('apple_data_transformed.csv', 'w', encoding=\"utf-8\", newline='') as file:\n",
    "    writer_obj = writer(file)\n",
    "    writer_obj.writerows(apple_data_free)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
